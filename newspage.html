\documentclass[journal]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{booktabs}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
% \usepackage{cite}
\usepackage{xcolor}      % 处理 \textcolor{red}{}
\usepackage{amssymb}     % 处理 \checkmark（对勾）
\usepackage{hyperref}  % 必须加载
\hypersetup{
  colorlinks=true,     % 超链接显示为彩色（非边框）
  linkcolor=blue,      % 内部链接颜色
  citecolor=green,     % 引用链接颜色
  urlcolor=black         % URL 颜色
}
\usepackage[style=ieee, doi=true]{biblatex} % 加载 biblatex 和 IEEE 样式
\addbibresource{main2.bib} % 指定 .bib 文件

% add ORCID
\usepackage{tikz,xcolor}
% \usepackage[implicit=false]{hyperref}
% \hypersetup{hidelinks,
% 	colorlinks=true,
% 	allcolors=black,
% 	pdfstartview=Fit,
% 	breaklinks=true}


\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{multirow}
\usepackage{booktabs}

\definecolor{lime}{HTML}{A6CE39}
\DeclareRobustCommand{\orcidicon}{
	\begin{tikzpicture}
		\draw[lime, fill=lime] (0,0)
		circle[radius=0.16]
		node[white]{{\fontfamily{qag}\selectfont \tiny \.{I}D}};
	\end{tikzpicture}
	\hspace{-2mm}
}
\foreach \x in {A, ..., Z}{%
	\expandafter\xdef\csname orcid\x\endcsname{\noexpand\href{https://orcid.org/\csname orcidauthor\x\endcsname}{\noexpand\orcidicon}}
}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\newcommand{\orcidauthorA}{0000-0002-1429-6653} % Add \orcidA{} behind the author's name zhaoming Wu
\newcommand{\orcidauthorB}{0000-0003-4293-6459} % Add \orcidB{} behind the author's name zhenchao chen
\newcommand{\orcidauthorC}{0009-0008-8716-7837} % Add \orcidC{} behind the author's name Yixiang Li
\newcommand{\orcidauthorD}{0009-0001-0500-2596} % Add \orcidD{} behind the author's name Zeqing Wang
\newcommand{\orcidauthorE}{0000-0002-2938-7419} % Add \orcidE{} behind the author's name Xuan Yang
\newcommand{\orcidauthorF}{0009-0003-0164-2494} % Add \orcidF{} behind the author's name Yulong Ding

\begin{document}

\title{ACOC-MT: More Effective Handling of Real-World Noisy Labels in Remote Sensing Semantic Segmentation}


% \author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
%         % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

\author{Zeqing Wang\orcidD{}, Yulong Ding\orcidF{}, Yixiang Li\orcidC{}, Zhaoming Wu\orcidA{}, Xuan Yang\orcidE{} and Zhengchao Chen\orcidB{}
\thanks{This research has been supported by the National Key Research and Development Program of China (2021YFB3901202). \textit{(Corresponding author: Zhengchao Chen.)}

Zeqing Wang, Zhaoming Wu and Yixiang Li are with the Key Laboratory of Remote Sensing and Digital Earth, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100101, China, and also with the University of Chinese Academy of Sciences, Beijing 100049, China (e-mail: wangzeqing22@mails.ucas.ac.cn;  liyixiang22@mails.ucas.ac.cn; wuzhaoming21@mails.ucas.ac.cn).

Yulong Ding received a B.S. degree in computer science and technology from Henan University of Technology, China, in 2022. He is currently a Master's student in Surveying Engineering at China University of Mining and 
Technology (Beijing), China.

Zhengchao Chen and Xuan Yang are with the Key Laboratory of Remote Sensing and Digital Earth, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100101, China (e-mail: yangxuan@radi.ac.cn, chenzc@aircas.ac.cn).


}

}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}

In remote sensing semantic segmentation, imperfect labels are prevalent due to the complexities of data acquisition and annotation processes. Although recent approaches to noisy label correction in remote sensing segmentation have shown promising results, challenges remain in accuracy and generalizability, given the incomplete consideration of the complex mixing involved. Nearly all methods should address three key challenges to identify and correct noisy labels: when to select labels, which labels to select, and how to handle the selected labels. We propose a novel label correction framework, \textbf{A}daptive \textbf{C}onsistency-Guided \textbf{O}bject-Level Label \textbf{C}orrection with \textbf{M}ean \textbf{T}eacher (ACOC-MT), which effectively addresses all three of these challenges. The ACOC-MT framework determines when to conduct label correction through the Stable Early Learning Detection module, selects noisy labels using the Consistency Threshold Mask module, and corrects the labels through the Object Label Correction module. To validate against real-world noisy labels rather than simulated ones, we constructed three real-world noisy-labeled datasets, CPCTC-N, BBD250-N, and CFD-N, from three different remote sensing scenarios. Extensive experiments on these three datasets demonstrate the efficacy and superior performance of our approach.
\end{abstract}

\begin{IEEEkeywords}
Remote sensing, deep learning, semantic segmentation, learning with noisy labels, real-world noisy labels, label correction
\end{IEEEkeywords}



\section{Introduction}
\IEEEPARstart{W}{ith} the rapid development of remote sensing (RS), computer vision (CV), and many other fields in recent years, the number of high-resolution remote sensing (HRRS) images has been growing explosively. It can be said that we have entered the big data era of HRRS images. The automatic interpretation of HRRS images can effectively reveal the value of the data, with applications in various fields such as military land analysis, environmental monitoring, agricultural remote sensing, urban planning, and even the planetary lidar field \cite{BROWN2015131}. Semantic segmentation is a key technique for HRRS image analysis and serves as a fundamental method for automatic interpretation of HRRS images \cite{lv2023deep}. Specifically, these techniques categorize each pixel in an image, enabling the delineation of various classes such as roads, cultivated land, buildings, and trees. In recent years, deep learning techniques have advanced significantly in remote sensing image segmentation, attracting increasing attention from researchers.

Deep learning supervised semantic segmentation models typically employ an encoder-decoder architecture, where images are encoded and decoded to produce segmentation results. To effectively train these models, a large, high-quality training dataset is required. Acquiring such datasets is time-consuming and costly, particularly due to the abundance of small-scale objects in HRRS images, which demands additional labeling effort. Therefore, despite having an abundance of HRRS, we face a significant shortage of high-quality labels \cite{li2023cost}.

Learning with noisy labels (LNL) aims to leverage noisy-labeled data to train models that can achieve good performance despite the presence of label noise. Thus, LNL methods effectively address the challenge of limited high-quality samples. In the field of RS, various sources of noisy labeled data can be utilized, broadly categorized into three types:

\begin{itemize}
\item Coarse annotations, including incomplete manual labels and various land use/land cover products (LULC), such as Google's Dynamic World \cite{brown2022dynamic}, ESA's WorldCover \cite{zanaga2021esa}, and ESRI's Land Cover \cite{karra2021global}, which provide large-scale but inherently noisy labels.
\item Crowdsourced data, which come from platforms such as OpenStreetMap (OSM) \cite{vargas2020openstreetmap} and Mapillary \cite{mapillary}, aggregate label information contributed by volunteers. Although these platforms offer freely accessible data, the annotations may be inconsistent.
\item Pixel-level pseudo-labels, which can be generated from weak labels, such as image-level tags, point annotations, and scribble-based labels. Common methods for generating these include class activation maps (CAM) \cite{zhou2016learning} and the Segment Anything Model (SAM) \cite{kirillov2023segment}.
\end{itemize}

\IEEEpubidadjcol

Numerous approaches have been developed to mitigate the effects of noisy labels and improve model performance. For example, Adaptive Early Learning Correction (ADELE) \cite{liu2022adaptive} improves robustness to noisy labels by introducing an adaptive early stopping mechanism. Uncertainty Estimation through Response Scaling for Noise Mitigation (URN) \cite{li2022uncertainty} identifies noisy labels through uncertainty estimation. Meta-learning-based methods \cite{zheng2021meta} assign lower weights to pixels whose loss gradient directions deviate significantly from clean data. In the field of RS, label correction methods are the mainstream approach in LNL semantic segmentation. Although recent label correction methods in RS have demonstrated strong performance in identifying specific features, such as high-resolution land cover mapping \cite{dong2021high}, green plastic cover (GPC) extraction \cite{cao2022coarse}, road extraction \cite{li2021exploring}, and building change detection \cite{cao2023full}-effectively mitigating the adverse effects of noisy labels remains a challenging task. Current research faces several important limitations:

\begin{enumerate}
 \item Most methods of noisy label correction face three key challenges in handling label noise: when to select labels, which labels to select, and how to handle the selected labels. Current approaches rely on empirical judgment to detect and correct label noise. Moreover, existing label correction algorithms are limited in their effectiveness and robustness because they seldom leverage RS object-level features for noise analysis and correction.

 \item Remote sensing imagery is distinct from natural scene images due to its complex characteristics, which are a result of varying imaging conditions and diverse object scales. Most LNL methods in RS rely on experiments with simulated noisy labels, which diverge from actual conditions due to the lack of real-world noisy labeled datasets.
 \end{enumerate}

To address the first limitation identified above, we propose an effective LNL semantic segmentation framework called Adaptive Confidence-Guided Object-Level Label Correction with Mean Teacher (ACOC-MT). This framework effectively mitigates the impact of noisy labels and enhances segmentation performance.
ACOC-MT has three key components, each targeting a key challenge in label noise correction. 
The Stable Early Learning Detection Module (SELD) addresses the question of when to select labels by leveraging training set performance metrics to detect overfitting and determine the optimal timing for activating label correction, as detailed in Section \ref{SELD}.
The Consistency Threshold Mask Module (CTM) corrects only unreliable labels, which are identified by their low prediction consistency between processing an image whole versus in parts, as detailed in Section \ref{CTM}.
The Object Label Correction Module (OLC) solves how to handle the selected labels by performing object-level corrections across all detected cases, utilizing spatial and structural information for more precise label refinement, as detailed in Section \ref{OLC}.
Additionally, the Mean Teacher method is incorporated to enhance model stability.
Our approach is network-agnostic, completely independent of clean labels, and compatible with LNL techniques leveraging data augmentation and regularization, making it adaptable to any major semantic segmentation network architecture, such as UNet, DeepLabv3+, and PSPNet.

For the second limitation, we curate three real-world noisy labeled datasets from distinct noise sources to support research on LNL semantic segmentation in RS.
For coarse annotations, we constructed the tree canopy segmentation dataset Chinese Provincial Capital City Tree Canopy-Noisy (CPCTC-N). For crowdsourced data, we derived the building segmentation dataset Bavaria Buildings Dataset 250-Noisy (BBD250-N) based on the Bavaria Buildings Dataset 250 (BBD250) \cite{werner2023bavaria}. For pixel-level pseudo-labels, we developed the flood segmentation dataset Calgary Flood Dataset-Noisy (CFD-N) from the Calgary Flood Dataset (CFD) \cite{he2024efficient}.

Overall, the main contributions of this research include:

\begin{enumerate}
 \item We propose a new framework for noisy semantic segmentation of RS images: Adaptive Confidence-Guided Object-Level Label Correction with Mean Teacher (ACOC-MT). This plug-and-play approach boasts a high degree of flexibility, rendering it compatible with any general semantic segmentation network. Moreover, our method does not require any clean data, making it highly practical for real-world noisy-label scenarios. 
\item In contrast to simulated noisy labels, we have curated three real-world noisy labeled datasets, namely Chinese Provincial Capital city Tree Canopy-Noisy (CPCTC-N), Bavaria Buildings Dataset 250-Noisy (BBD250-N), and Calgary Flood Dataset-Noisy (CFD-N), which were derived from three distinct noise sources.
\item We conducted a large number of experiments on the three datasets, demonstrating that the ACOC-MT method achieved competitive results compared to existing architectures. Compared to the baseline method, our approach achieved improvements in the IoU of 1.43\%, 2.75\%, and 3.7\% on the CPCTC-N, BBD250-N, and CFD-N datasets, respectively.
  \end{enumerate}

The structure of this paper is as follows: Section II reviews related studies on LNL in remote sensing semantic segmentation. Section III provides a detailed explanation of the proposed ACOC-MT framework. Section IV describes the construction of three real-world noisy label datasets and presents experimental results and analysis. Finally, Section V concludes the paper with discussions and potential future research directions.

\section{RELATED WORKS}
LNL is a significant issue in the field of CV and has garnered considerable attention in RS in recent years. Given the remarkable performance of deep learning methods in attaining state-of-the-art results, our primary focus lies in the synthesis of LNL methods within the context of deep learning. These methods can be comprehensively classified into three primary categories.

\subsection{Noise Transition Matrix Estimation}
The noise transition matrix estimation method addresses label noise by simulating the noise process by learning the flipping probabilities between true labels and noisy labels. 
As Sukhbaatar et al. \cite{sukhbaatar2014training} demonstrate, a learnable linear noise layer can be designed to represent the transition matrix. This layer is integrated with the softmax layer to model the label corruption process. 
Srivastava et al. \cite{srivastava2014dropout} build upon this approach by incorporating a Dropout-based regularization technique into the noise adaptation layer. This technique improves its robustness against label noise. 
Goldberger et al. \cite{goldberger2017training} add another softmax layer and initialize the transition matrix with the pre-trained parameters. 
Patrini et al. \cite{patrini2017making} estimate the transition matrix based on the network output trained with noisy labels and use this matrix for loss correction. 
Gold Loss Correction (GLC) \cite{hendrycks2018using}, extending existing robust label noise methods, further refines the transition matrix estimation by incorporating clean score information derived from the training data.
Xiao et al. \cite{xiao2015learning} present a system that trains two networks: one to predict the transition matrix and the other to indicate the noise type. This system simulates label noise uncertainty by adding multivariate normal latent variables to the classifier's final hidden layer.
Addressing the issue of noisy GIS-derived labels for building extraction, Zhang et al. \cite{zhang2020gis} introduced a Probability Transition Module (PTM) specifically designed to capture the relationship between true and noisy labels.
Ahmed et al. \cite{ahmed2021dense} extract buildings by estimating the noise distribution through error approximation, but their method requires the support of an accurate dataset. 
 
The disadvantage of the noise transfer matrix estimation method is that it treats all training samples equally and fails to identify mislabeled samples, which can lead to errors in estimation \cite{xia2022extended}. In contrast, our approach corrects noise at the object level, filtering clean and noisy sample objects through the network's learning process.

\subsection{Noise-Applicable Regularization}
Initially, the CV field used typical regularization techniques to reduce overfitting during training, such as dropout, data augmentation, and weight decay. 
Regularized self-labeling (REGSL) \cite{li2021improved} showed that these techniques can also improve generalization performance in the presence of noisy labels. Display regularization is a method that enables the modification of the loss during the training process.
The dual-layer learning method \cite{jenni2018deep} employs a clean validation dataset to address the dual-layer optimization regularization issue, utilizing cross-validation principles.
The annotator confusion method \cite{tanno2019learning} assumes the presence of multiple annotators and incorporates a regularization term into the loss function, driving the matrix to approximate the true annotator confusion matrix.
Partial Huberisation (PHuber) \cite{menon2020can} proposed a loss-based composite gradient clipping method for label noise. 
Xia et al. \cite{xia2020robust} proposed a method that classifies key and non-key parameters when fitting clean and noisy labels, applying different regularization strategies to mitigate the memorization effect of noisy labels.
Implicit regularization gives some random effects that indirectly guide model optimization. 
Adversarial learning \cite{goodfellow2014explaining} enhances the model’s tolerance to noise by training it on intentionally perturbed "adversarial examples," which forces the model to learn more robust features rather than overfitting to noise.
Mixup \cite{zhang2017mixup} trains neural networks using virtual samples generated from a linear combination of randomly selected pairs of training images and their corresponding labels. 
Label smoothing \cite{lukasik2020does} explores whether the regularization technique of label smoothing remains applicable under label noise conditions.
Adaptive Label Smoothing via Auxiliary Classifier (ALASCA) \cite{ko2023gift} employs an adaptive label smoothing technique along with an auxiliary classifier to obtain robust features.

Noise-applicable regularization methods have some main drawbacks: they may reduce the model's convergence speed and their performance on complex tasks such as semantic segmentation may be limited due to resource constraints. In our study, we did not use these methods to improve model performance; however, it is important to note that our approach is complementary to these methods and can be used in conjunction with them.


\subsection{Label Selection/Correction}
Label-based methods focus on the identification and correction of noisy labels.
The memorization effect has been observed in many studies \cite{arpit2017closer}, \cite{liu2020early}, \cite{song2019does}, where the neural network is observed to first learn meaningful patterns from the majority of data before eventually memorizing and overfitting to the mislabeled data in later training stages. Methods based on label selection/correction leverage the memorization effect to identify clean samples from noisy ones for training. Almost all such methods aim to address three key questions: when to select labels, which labels to select, and how to handle the selected labels.

Regarding the question of determining when to make sample selections, most studies have not specifically explored this issue. Instead, they either select samples at each training epoch or rely on empirical experience. Liu et al. \cite{liu2022adaptive} fits the training accuracy curve to an exponential function to monitor gradient changes. Song et al. \cite{song2022self} utilizes the rate of change in validation set loss, while Adaptively triggered Online Object-wise correction (AIO2) \cite{liu2024aio2} and Peaks fusion assisted Early-Stopping (PEAS) \cite{liu2022peaks} fit the training using mean Intersection over Union (mIoU).

Regarding the question of which samples to choose, mainstream approaches utilize loss to identify noisy samples, such as Co-teaching \cite{han2018co}, Co-teaching+ \cite{yu2019does}, and Joint Training with Co-Regularization (JoCoR) \cite{wei2020combating}. Some methods, such as the approach proposed by Cao et al. \cite{cao2022coarse}, identify noisy samples by considering those with low prediction probabilities. Sun et al. \cite{sun2022mutual} alternates between group confidence-based and loss-threshold-based selection. AIO2 \cite{liu2024aio2} addresses only the missed detection issue, identifying areas where prediction results deviate from noisy labels as noise. Additionally, entropy is used by \cite{dong2021high} to measure the uncertainty of predicted samples, while the variance of results across multiple epochs is employed by \cite{li2021exploring} to assess sample uncertainty.

Regarding how to deal with the selected samples, some methods opt to directly remove the noisy samples \cite{song2022self}, \cite{sun2022mutual}, \cite{zhang2023agreement}.   Alternatively, other approaches replace the noisy samples with predicted labels \cite{dong2021high}, \cite{cao2022coarse}, \cite{li2021exploring}, \cite{cao2023full}. Liu et al. \cite{liu2022peaks} propose using the average of predictions from multiple epochs as a substitute for noisy samples. Henry et al. \cite{henry2021aerial} employ both the sources and predicted labels for weighting purposes, while AIO2 \cite{liu2024aio2} applies a smoothing filter to the predicted labels, generating soft boundaries.

Our method addresses the three questions mentioned above. The Stable Early Learning Detection Module determines the optimal timing for label modification by fitting the training set's mean Intersection over Union (mIoU). The Consistency Threshold Mask Module generates consistency scores for each labeled object based on the varying outputs of the network, allowing us to distinguish between noisy and clean samples. For the noisy samples, we employ the Object Label Correction Module, which takes into account all object-level scenarios for correction. 

The work most similar to ours is AIO2 \cite{liu2024aio2}, as both utilize a Student-Teacher paradigm. The fundamental difference, however, lies in how each method identifies unreliable labels, a distinction motivated by our focus on tackling diverse real-world noise. AIO2's correction mechanism uses teacher-model predictions to find and add omitted objects, a design tailored for simulated incomplete labels (i.e., false negatives). In contrast, our novel CTM module performs a self-consistency check by comparing full-image versus partial-image predictions. This mechanism allows our framework to be more general, making it capable of handling a wider variety of real-world noise that includes both false positives and false negatives. Furthermore, because this consistency check is class-agnostic, our framework naturally generalizes to challenging multi-class segmentation tasks.










\section{Method}

For clarity, we first outline the notation used. 
In a typical problem of semantic segmentation, a clean training dataset $\mathcal D=\left\{ (x_i, y_i) \right\}_{i=1}^N$ is provided, where \( x_i \in \mathbb{R}^{ W\times  H\times C} \) denotes the \( i \)-th image, $y_i \in \mathbb{R}^{W \times H}$ represents the clean label, and  $N$ is the number of samples in the dataset. 
Let $\mathcal F(:,\theta)$ denote the neural network model parameterized by $\theta$. 
Our objective is to optimize the model by minimizing the loss function \( \mathcal L(\mathcal F(x, \theta), y) \), where \( \mathcal F(x, \theta) \) represents the model predictions given input \( x \) and parameters \( \theta \). 
In LNL, the true label $y$ is unknown, and we only have a noisy labeled training dataset $\tilde{\mathcal D}=\{ (x_i, \tilde{y}_i) \}_{i=1}^N$, where $\tilde{y}_i$ represents the noisy label for each $x_i$ in the dataset. 
Training deep neural networks directly with noisy labels often causes the model to learn incorrect information, reducing its ability to capture true category patterns and significantly harming performance. To address this, we propose a label correction framework called Adaptive Confidence-Guided Object-Level Label Correction with Mean Teacher (ACOC-MT). The details of our approach are provided below.

\subsection{Overview of the Framework}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\textwidth]{fig1.pdf} 
  \caption{The framework of our approach ACOC-MT. During the initial Early Learning Phase, the student model learns by treating noisy labels as entirely correct, updating its parameters accordingly. Meanwhile, the teacher model is updated exclusively through Exponential Moving Average (EMA) without applying backpropagation. At the same time, the Stable Early Learning Detection Module monitors the training set accuracy. Once it detects that the model begins to fit noisy labels, the training moves to the Correcting Learning Phase. During the Correcting Learning Phase, the Consistency Threshold Mask Module uses the predictions of the teacher model to identify instances that require correction and generates an Object Consistency Mask. The consistency mask is visualized, where green indicates high-consistency objects and red indicates low-consistency objects. The Object Label Correction Module then refines the noisy labels at the object level, producing high-quality corrected labels that guide the student model's learning process.}
  \label{fig:1}
\end{figure*}

The ACOC-MT framework, illustrated in Fig.\ref{fig:1}, is divided into two main phases: the Early Learning Phase and the Correcting Learning Phase. In the Early Learning Phase, the training of the model will be done with the input label noise as reference data. Initially, the model focuses on clean labels to enhance performance. When it starts to overfit the noisy labels, the Stable Early Learning Detection Module (SELD) triggers and the model transitions into the Correcting Learning Phase. The Correcting Learning Phase allows the model to perform label correction before each training round, replacing noisy labels with corrected ones to guide the model until convergence.
Label correction consists of two steps: the Consistency Threshold Mask Module (CTM), and the Object Label Correction Module (OLC). The CTM module generates a consistency threshold mask for the predicted labels to detect potentially noisy labels.
The OLC module then corrects these noisy labels at the object level, resulting in higher-quality corrected labels.
Throughout the training process, we incorporate a teacher model whose weights are updated using an exponential moving average (EMA) \cite{laine2016temporal} of the historical weights from the student model. 
The introduction of the teacher model has two main benefits: first, it provides a smoother training accuracy curve, which helps the early learning module to make more accurate judgments about early stopping points; second, it produces more stable predictions. These stable predictions are used to generate corrected labels which, while not perfect, are demonstrably superior to the original noisy labels in aggregate.  

Our framework is designed to initiate a virtuous cycle through the synergy of its core components. The SELD module determines the opportune moment to begin correction, ensuring the model's predictions are sufficiently reliable. Subsequently, the CTM module precisely identifies which labels are unreliable based on our self-consistency principle. Finally, the OLC module applies a more stable correction using the teacher model's predictions. This coordinated process progressively refines the training data, guiding the model towards a more robust and accurate solution.

\subsection{Stable Early Learning Detection Module}
\label{SELD}
Due to the memorization effect, noisy labels begin to significantly degrade the model's performance in the later stages of the training process. 
Therefore, a logical approach is to start the label correction process at the end of the Early Learning Phase.
We define the end of the Early Learning Phase as the stop point $E_{stop}$, where the model achieves optimal performance on clean labels without fitting any noisy labels. We developed a Stable Early Learning Detection (SELD) Module to find $E_{stop}$.

Inspired by \cite{liu2022adaptive}, we employ a fitting strategy to identify $E_{stop}$. As illustrated in Fig.\ref{fig:2}, when the Intersection over Union (IoU) of the validation dataset begins to decline, the growth rate of the IoU for the training dataset correspondingly slows down.
Thus, we can determine the stop point $E_{stop}$ by monitoring the rate of increase in the training IoU. 
Specifically, at the $n$-th epoch, we apply the least squares method to fit the IoU of the training dataset over the previous $n$ epochs to the following exponential function:
\begin{equation}
 f(n) = a \cdot (1 - e^{-b \cdot n^c}) 
\label{equation1}
\end{equation}
where \( 0 < a \leq 1 \), \( b \geq 0 \), and \( 0 < c \leq 1 \) are fitting parameters.
Accordingly, we obtain
\begin{equation}
 f'(n) = abc \cdot n^{c-1} \cdot e^{-b \cdot n^c}
\label{equation2}
\end{equation}

We can measure the deceleration $g(n)$ by comparing the gradient at each epoch with the gradient at the first epoch.

\begin{equation}
 g(n) = \left| \frac{f'(1)-f'(n)}{f'(1)}\right|
\label{equation3}
\end{equation}

\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{fig2.png}
   \caption{In complex real-world remote sensing scenarios, previous early learning detection methods often make judgments either too early or too late (as indicated by the candidate stop points in the figure), impacting the noise correction process.}
   \label{fig:2}
\end{figure}

In \cite{liu2022adaptive}, the stop point is determined as epoch $n$ when $g(n)>r$, where $r$ is a fixed threshold.
However, real remote sensing scenarios involve complexities that lead to noticeable fluctuations in training accuracy during early training. This can cause the stopping point to be detected too early, before the true end of the early learning phase.

To address this issue, we implement a stability detection mechanism:
When \( g(n) > r \) at epoch $n$, $n$ is designated as a candidate stopping point, and training continues. We only confirm 
$n$ as a stable stopping point after it has accumulated as a candidate for 
$m$ cumulative times, where $m$ is a hyperparameter (i.e., a parameter whose value is set before the learning process begins). The advantage of this stability detection mechanism is that training is terminated only when a given epoch is consistently identified as a stopping point across subsequent training iterations. This approach helps eliminate interference caused by inherent randomness in the training process, thereby enhancing robustness.



\subsection{Consistency Threshold Mask Module}
\label{CTM}
The Consistency Threshold Mask Module (CTM) generates a consistency mask for the predicted labels, as illustrated in Fig.\ref{fig:3}. The core motivation of the CTM module is to leverage the principle of contextual invariance to quantify prediction reliability. If a model's predictions for an object remain consistent when viewed with both global (full-image) and local (partial-image) context, the prediction is considered reliable.

\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{fig3.pdf}
   \caption{Diagram of the Consistency Threshold Mask Module, showing how the consistency mask for each object is obtained by measuring consistency across different prediction results. The consistency mask is visualized, where green indicates high-consistency objects and red indicates low-consistency object.}
   \label{fig:3}
\end{figure}

To implement this, we input the complete image \( \mathcal{I}_{f} \) into the network to obtain the prediction \( \mathcal{Y}_{f} \). Then, we divide the image into four parts and input each part \( \mathcal{I}_{p} \) separately into the network. Each part is resized to the original input dimensions before being processed. We merge the resulting predictions as \( \mathcal{Y}_{p} \). It is worth noting that we add a 32-pixel margin of overlap along the image edges to prevent prediction degradation at the boundaries. This overlapping margin is automatically trimmed from each predicted part before they are merged into the final seamless result.

Before calculating consistency, it is necessary to first establish the correspondence between object instances in $\mathcal{Y}_f$ and $\mathcal{Y}_p$. To this end, we employ a reference-based matching strategy. First, the set of object instances $\mathcal{O}=\{O_i\}$, extracted from the full-image prediction $\mathcal{Y}_f$ via connected-component analysis, is treated as the reference set with a fixed index $i$. Then, for each reference object $O_i$ and its corresponding mask $\mathcal{A}_i^f$, we search through all object masks detected in the partial-image prediction $\mathcal{Y}_{p}$ to find the one with the maximum spatial overlap (i.e., the largest pixel intersection). This identified mask is then defined as $\mathcal{A}_i^p$. If no mask in $\mathcal{Y}_{p}$ overlaps with $\mathcal{A}_i^f$, $\mathcal{A}_i^p$ is considered an empty set, resulting in a zero intersection. This process ensures that every $\mathcal{A}_i^f$ has a uniquely determined counterpart $\mathcal{A}_i^p$ for the subsequent calculation.

After establishing the correspondence for all objects, we assess the consistency of each object \( O_i \) by calculating the pixel overlap between \( \mathcal{Y}_{f} \) and \( \mathcal{Y}_{p} \). Specifically, we define the consistency metric \( \mathcal{C}_i \) for object \( O_i \) as:

\begin{equation}
\mathcal{C}_i = \frac{| \mathcal{A}_i^f \cap \mathcal{A}_i^p |}{| \mathcal{A}_i^f \cup \mathcal{A}_i^p |}
\label{equation4}
\end{equation}

where $\mathcal{A}_i^f$ is the mask for the reference object $O_i$ in $\mathcal{Y}_{f}$ and $\mathcal{A}_i^p$ is its corresponding mask in $\mathcal{Y}_{p}$, identified as described above.

We compute an adaptive threshold, $\tau_{\text{ada}}$, which is defined as the median of the consistency scores from all $N$ objects, $\{\mathcal{C}_1, \mathcal{C}_2, \dots, \mathcal{C}_N\}$, within a given image:

\begin{equation}
\tau_{\text{ada}} = \text{median}(\{\mathcal{C}_1, \mathcal{C}_2, \dots, \mathcal{C}_N\})
\label{eq:tau_ada_definition}
\end{equation}

This adaptive strategy bolsters the method's robustness by tailoring the threshold to scene-specific characteristics, including object density and prediction complexity. The median is specifically employed for its inherent robustness to outlier consistency scores, ensuring a more stable estimate of the typical consistency level compared to the mean.

Using this threshold, we define the consistency mask $\mathcal{M}$ as follows:

\begin{equation}
\mathcal{M}(i) = \begin{cases}
1 & \text{if } \mathcal{C}_i > \tau_{\text{ada}}, \\
0 & \text{if } \mathcal{C}_i \le \tau_{\text{ada}},
\end{cases}
\label{equation5}
\end{equation}

where $\mathcal{C}_i$ represents the pixel-level consistency score of object $O_i$.

Based on this score, $\mathcal{M}(i) = 1$ indicates high consistency, while $\mathcal{M}(i) = 0$ indicates low consistency. If an object has high consistency, it is likely that the predictions for that object are more reliable and that the object may represent a simpler sample. Conversely, if the object has low consistency, this suggests lower reliability in the predictions, which may also indicate that the object is a more challenging sample to label.



\subsection{Object Label Correction Module}
\label{OLC}
The Object Label Correction (OLC) module is designed to selectively refine the noisy labels. For a given object $O_i$, the module makes a correction decision based on three key inputs: its original noisy label mask ($\mathcal{A}_i^n$), the stable full-image prediction mask from the teacher model ($\mathcal{A}_i^f$), and its reliability as determined by the consistency mask ($\mathcal{M}_i$). The final corrected label of object $O_i$, denoted as $\mathcal{A}_i^c$, is defined as follows:

\begin{equation}
\mathcal{A}_i^c =
\begin{cases}
\mathcal{A}_i^f, & \text{if } (\mathcal{A}_i^n \cap \mathcal{A}_i^f \neq \emptyset \text{ and } \mathcal{M}_i = 0) \\
& \text{or } (\mathcal{A}_i^f \neq \emptyset \text{ and }  \mathcal{A}_i^n = \emptyset \text{ and } \mathcal{M}_i = 1), \\
\mathcal{A}_i^n, & \text{if } (\mathcal{A}_i^n \cap \mathcal{A}_i^f \neq \emptyset \text{ and } \mathcal{M}_i = 1) \\
& \text{or } (\mathcal{A}_i^n \neq \emptyset \text{ and }  \mathcal{A}_i^f = \emptyset) \\
& \text{or } (\mathcal{A}_i^f \neq \emptyset \text{ and }  \mathcal{A}_i^n = \emptyset \text{ and } \mathcal{M}_i = 0), \\
\end{cases}
\label{equation6}
\end{equation}


The specific details are shown in Fig.\ref{fig:4}. In this process, the decision to update the label is based on the consistency of the object between the noisy label and the prediction, as well as the object's appearance in the two sources. Specifically, we consider the following conditions for each object:

\begin{enumerate}
\item When the object is present in both the noisy label and the prediction with low consistency, it likely represents a noisy and challenging sample. In this case, the prediction is utilized for label correction.
\item When the object appears in both the noisy label and the prediction with high consistency, it suggests a relatively simple sample, though potential boundary inaccuracies may exist in the model. Therefore, the noisy label is retained to provide better boundary supervision.
\item If the object is present only in the noisy label, the original label is preserved regardless of consistency, as further training is expected to guide the model toward cases 1) or 2).
\item If the object appears only in the prediction with low consistency, it is likely a difficult sample prone to misclassification. To mitigate continued misclassification, the noisy label is maintained.
\item If the object appears only in the prediction with high consistency, it likely indicates a missed detection in the noisy label. Thus, the prediction is adopted for label correction.
\end{enumerate}

\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{fig4.pdf}
   \caption{The decision logic of the Object Label Correction (OLC) module. The corrected label ($\mathcal{A}^c$) for an object is generated by selectively choosing between the original noisy label ($\mathcal{A}^n$) and the teacher's full-image prediction ($\mathcal{A}^f$), guided by the object's reliability score from the consistency mask ($\mathcal{M}$).}
   \label{fig:4}
\end{figure}



\section{Experiments}

\subsection{Datasets}

\subsubsection{Our Curated Real-World Noisy Datasets}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.74\textwidth]{fig5.png} 
  \caption{The details of our curated three real-world noisy labeled datasets in RS are as follows: (a) CPCTC-N, (b) BBD250-N, and (c) CFD-N. The upper part of the figure illustrates the training set, which includes images and noisy labels, while the lower part presents the validation set, consisting of images and clean labels. It is also worth noting that even these "clean" labels may not be perfectly accurate, as the segmentation of natural structures is inherently ambiguous at the boundaries.}
  \label{fig:buchong_1}
\end{figure*}


In the field of CV, there are numerous real noisy datasets available for study, such as ANIMAL-10N \cite{song2019selfie}, CIFAR-10N \cite{wei2021learning}, Food-101N \cite{lee2018cleannet}, Clothing1M \cite{xiao2015learning}, WebVision \cite{li2017webvision}, and so on. However, in the field of RS, most experiments are conducted based on simulated noisy labeled data. We believe that the existence of real-world noisy labeled datasets in RS is highly beneficial.

Real-world noisy labeled data in remote sensing can be broadly classified into three categories: (1) coarse annotations, which are often inaccurate when produced by traditional methods; (2) crowdsourced data, such as that from OpenStreetMap (OSM), collected through public collaboration and resulting in a large volume of labels, albeit with variable quality; and (3) pixel-level pseudo-labels derived from weak annotations (e.g., image-level, point-level, or sketch-level labels), primarily utilized in weakly supervised segmentation tasks such as weakly supervised semantic segmentation (WSSS). For each of these three categories, we have collected and curated real-world noisy labeled datasets.

\textbf{CPCTC-N(Chinese Provincial Capital city Tree Canopy-Noisy)}: To create a real-world noisy labeled dataset based on coarse segmentation results, we first segment the unlabeled image pixels into distinct objects using an object-oriented, multi-scale segmentation approach. After assigning labels to a limited set of objects manually, we apply an object-oriented nearest neighbor classification algorithm to categorize all remaining objects and generate the dataset. Using this method, we constructed a real-world noisy labeled dataset focused on urban tree canopy extraction, named the Chinese Provincial Capital city Tree Canopy-Noisy (CPCTC-N) dataset. The image data were sourced from the GF2 satellite, drawn from urban construction areas in provincial capitals across China, with a temporal range from 2018 to 2022. This dataset comprises 2,586 images of 1024x1024 pixels, including 2,070 images in the training set and 516 images in the validation set. The details of CPCTC-N are shown in Fig.\ref{fig:buchong_1}(a). Apart from edge detail issues, the primary source of label noise in this dataset stems from false positives, such as those caused by water bodies or shadows.

\textbf{BBD250-N(Bavaria Buildings Dataset 250-Noisy)}: Unlike the abundance of existing crowdsourced datasets, the dataset needed for our research involves noisy labels in the training set and clean labels in the validation set. To create this, we developed a real-world noisy labeled dataset from crowdsourced data based on the Bavaria Buildings Dataset 250 (BBD250 \cite{werner2023bavaria}). For each image, it provides two segmentation masks: one based on official building footprints (Hausumringe) from the Free State of Bavaria and another based on OSM extracts from 2021. This setup facilitates the comparison of labels between official data and OSM, enabling us to identify clean labels. For the validation labels, we selected images where the intersection-over-union between the official labels and OSM labels exceeded 95\%, considering these as clean labels. For the training labels, we randomly sampled from OSM data to ensure the dataset's distribution reflects real-world conditions. This process resulted in the BBD250-N dataset, which contains 33,935 images of 250 × 250 pixels, including 31,054 images in the training set and 2,881 images in the validation set. The details of BBD250-N are shown in Fig.\ref{fig:buchong_1}(b). Apart from edge detail issues, the primary source of label noise in this dataset arises from temporal mismatches between the data and the crowdsourced labels.

\textbf{CFD-N(Calgary Flood Dataset-Noisy)}: The primary methods for generating pixel-level pseudo-labels include random seed expansion, categorical activation maps, etc. Given the strong performance and increasing application of the Segment Anything Model (SAM), we aim to create a real-world noisy labeled dataset with pixel-level pseudo-labels using SAM. The Calgary Flood Dataset (CFD) \cite{he2024efficient} is based on the 2013 flood in Calgary, Canada, covering a densely populated urban area with diverse land cover types, such as commercial, residential, and industrial regions. The dataset includes two types of training data: fully labeled data and weakly labeled data. For the training labels, we used the weakly labeled data generated with SAM, while for the validation labels, we selected the fully labeled data, filtering out flood-free images. This process resulted in the CFD-N dataset, containing 1,901 images at a resolution of 512 × 512 pixels, including 1,055 images for training and 846 images for validation. The details of CFD-N are shown in Fig.\ref{fig:buchong_1}(c). Apart from edge detail issues, the primary source of label noise in this dataset is mainly attributed to missed detections caused by the suboptimal performance of the SAM model.

\subsubsection{Public Benchmarks for Additional Validation}
To further conduct a well-controlled study and test the generalizability of our framework, we also employed two public benchmark datasets, one for evaluating robustness against artificial noise and the other for a multi-class segmentation task.

\textbf{Massachusetts Buildings Dataset}: To conduct a well-controlled study with artificial noise, we utilized the public Massachusetts Buildings Dataset \cite{mnih2013machine}. This dataset is composed of 151 RGB aerial images of the Boston area at 1m spatial resolution, with corresponding high-quality building footprint masks, and is officially divided into training, validation, and test sets. 

For our controlled study, we used the official training split and first cropped the images into a series of 256$\times$256 patches. After removing any patches that contained no building labels, a final set of 2,758 patches was used for training. From these clean labels, we then simulated "incomplete label noise" by randomly dropping 30\%, 50\%, and 70\% of the building instances. For evaluation, we merged the official validation and test sets to create a larger clean reference set. These images were similarly cropped, yielding a final clean validation set of 230 patches on which all models were assessed.

\begin{table*}[h!]
\centering
\caption{Summary of the datasets used in our experiments. The upper panel details our three curated real-world noisy datasets, while the lower panel describes the two public benchmarks used for additional validation.}
\label{tab:dataset_summary}
\resizebox{\textwidth}{!}{
    \begin{tabular}{@{}lccccccl@{}}
    \toprule
    \textbf{Dataset} & \textbf{Train Smpl.} & \textbf{Val Smpl.} & \textbf{Resolution} & \textbf{Patch Size} & \textbf{Bands} & \textbf{Platform} & \textbf{Noise Type} \\ 
    \midrule
    
    CPCTC-N & 2,070 & 516 & 0.8m & 1024$\times$1024 & RGB-NIR & GF2 & Real (Coarse Annotation) \\ 
    BBD250-N & $\sim$31k & 2,881 & 0.4m & 250$\times$250 & RGB & Airplane & Real (Crowdsourced) \\ 
    CFD-N & 1,055 & 846 & 0.2m & 512$\times$512 & RGB & Airplane & Real (Pseudo-Label) \\ 
    \midrule
    
    Massachusetts & 2,758 & 231 & 1.0m & 256$\times$256 & RGB & Airplane & Artificial \\
    NoLDO-S12 & $\sim$20.5k (Subset) & 340 & 10m & 264$\times$264 & RGB & Sentinel-2 & Real (Pseudo-Label) \\
    \bottomrule
    \end{tabular}
    }
\end{table*}

\textbf{NoLDO-S12 Dataset}: To evaluate the generalization capability of our ACOC-MT framework for multi-class segmentation, we utilized the publicly available NoLDO-S12 dataset \cite{liu2025cromss}. This benchmark is specifically designed with a large-scale pre-training dataset containing noisy labels (SSL4EO-S12@NoL) and two clean downstream multi-class segmentation datasets for evaluation (SSL4EO-S12@DW and SSL4EO-S12@OSM). The noise labels in this dataset are sourced from the Google Dynamic World project and are also a type of pseudo labels generated by automated models. In our experiments, we used single-modality, three-band RGB Sentinel-2 data. Due to the large scale of the dataset and our computational constraints, we trained our models on a randomly sampled subset of 20,498 images from the noisy SSL4EO-S12@NoL dataset. For evaluation, we used the official clean validation set of the SSL4EO-S12@DW component, which consists of 340 images.
\\
\indent Table \ref{tab:dataset_summary} presents the detailed parameter information of the five datasets.



\subsection{Metrics}


For the evaluation of model performance, we employ several common metrics, including Precision (P), Recall (R), Intersection over Union (IoU), and F1 score. 
Additionally, to assess computational efficiency, we measure throughput.
These metrics are defined as follows:

\textbf{Precision (P)} is the proportion of true positive predictions among all positive predictions, representing the accuracy of the model's positive class predictions. It is defined as:
\begin{equation}
P = \frac{TP}{TP + FP}
\end{equation}
where \( TP \) (True Positive) is the number of pixels correctly predicted as positive, and \( FP \) (False Positive) is the number of pixels incorrectly predicted as positive.

\textbf{Recall (R)} is the proportion of true positive predictions among all actual positive instances. It evaluates the model's ability to correctly identify positive pixels. It is given by:
\begin{equation}
R = \frac{TP}{TP + FN}
\end{equation}
where \( FN \) (False Negative) represents the number of positive pixels that were incorrectly predicted as negative.

\textbf{Intersection over Union (IoU)} is used to measure the overlap between the predicted and ground truth masks. It is defined as the ratio of the intersection to the union of the predicted and actual positive pixels:
\begin{equation}
IoU = \frac{TP}{TP + FP + FN}
\end{equation}

A higher IoU value indicates a higher degree of overlap between the predicted result and the true label, which provides a better segmentation performance of the model.

\textbf{F1 Score} is the harmonic mean of Precision and Recall, which provides a balanced measure of model performance. It is calculated as:
\begin{equation}
F1 = 2 \times \frac{P \times R}{P + R}
\end{equation}

\textbf{Throughput (FPS)} is used to measure computational efficiency. This metric is defined as the number of images processed per second (Frames Per Second, FPS) and was benchmarked on the hardware specified in Section \ref{Experimental Settings}.


These metrics provide a comprehensive assessment of the model's segmentation accuracy and efficiency. In our evaluation process, we primarily focus on IoU and F1 Score, while also considering Precision and Recall to analyze the model's behavior under different noise conditions. To avoid interference from a large number of background pixels, we focus solely on the foreground classes in our calculations.



\subsection{Experimental Settings}
\label{Experimental Settings}
We conducted our experiments on the PyTorch platform using a single NVIDIA 3090 GPU, employing the widely adopted U-Net \cite{ronneberger2015u} architecture across all datasets. The optimizer chosen was AdamW, with an initial learning rate of 1e-4 and a weight decay of 0.01. The batch size was set to the maximum value that the GPU memory could accommodate to ensure efficient utilization of computational resources.

To ensure the statistical significance of our results, all reported metrics are the mean and standard deviation from 3 independent runs with different random seeds. 
Common remote sensing techniques were applied for data augmentation, including flip, transpose, Hue, Saturation, Value (HSV) adjustment, shift-scale-rotate, and grid distortion.

\subsection{Comparison with State-of-the-Art Methods}

\textbf{Quantitative Results} Since some recent algorithms designed for Learning with Noisy Labels (LNL) in image classification may not be directly applicable to semantic segmentation, we selected a set of representative algorithms that are suitable for comparison in this task:
(1) standard training \cite{ronneberger2015u}, 
(2) Co-Teaching \cite{han2018co}, 
(3) JoCoR \cite{wei2020combating}, 
(4) CORES$^2$ \cite{cheng2020learning}, 
(5) Peer Loss \cite{liu2020peer}, 
(6) Bootstrap \cite{henry2021aerial}, 
(7) COPLE \cite{katore2022noise}, 
(8) ADELE \cite{liu2022adaptive},
(9) L2B \cite{zhou2024l2b} and
(10) AIO2 \cite{liu2024aio2}. 
To ensure a fair comparison, we report the best performance achieved by each method. We use Intersection over Union (IoU), F1 score, Precision (P), and Recall (R) as our performance metrics. Our approach demonstrates highly competitive results across all datasets, as discussed below.

The results for the CPCTC-N dataset are shown in Table \ref{tab:method_comparison_cpctc_new_data}, which indicate that our proposed ACOC-MT achieves the best overall performance, attaining the highest scores in both IoU (82.53\%) and F1-score (90.42\%).

The noise in this dataset, originating from coarse segmentation outputs, exhibit characteristics such as a moderate level of noise, discernible noise patterns, and a notable presence of false positives.
In this case, noise robust regularization methods such as Peer Loss and COPLE perform poorly, suggesting that these algorithms are not well-suited for noisy labels derived from coarse segmentation results. Co-Teaching and JoCoR perform relatively well, while label correction methods yield the best results, indicating that label correction approaches more effectively handle real-world noisy labeled datasets from coarse segmentation outputs.
Additionally, we observe that Co-Teaching achieves the highest Precision, while L2B attains the highest Recall. However, their excessive focus on a single aspect constrains their overall performance. In contrast, our ACOC-MT achieves a superior balance between precision and recall. This can be attributed to our consistency-based correction mechanism, which effectively identifies and handles false positives characteristic of this dataset without sacrificing recall.


The results for the BBD250-N dataset are shown in Table \ref{tab:method_comparison_bbd250}, which indicate that our proposed ACOC-MT achieves the best overall performance, attaining the highest scores in both IoU (89.15\%) and F1-score (94.26\%). 

The noise in this dataset, originating from crowdsourced data, exhibit characteristics such as a relatively small amount of noise, often concentrated at the boundaries. In this case, methods based on loss selection or weighting, such as Co-Teaching, JoCoR, and CORES$^2$, show minimal performance improvement, suggesting that these algorithms struggle to accurately identify noisy labels in crowdsourced data with low noise levels. Due to an incorrect estimation of the correction timing, ADELE and L2B adopted inappropriate predictions as correction labels, leading to inferior performance compared to the standard approach. Peer Loss and COPLE achieve relatively better performance, indicating that noise-robust regularization methods are more suitable for noisy labels in crowdsourced data. 
Our approach applies object-level correction to adjust label noise while effectively preserving boundary information, achieving optimal performance.

\begin{table*}[t!]
\centering
\caption{Performance Comparison on the CPCTC-N Dataset. All metrics are presented as Mean ± Std (\%). The \textbf{best} and \underline{second-best} results are highlighted.}
\label{tab:method_comparison_cpctc_new_data}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{l c c c c}
    \toprule
    \textbf{Method} & \textbf{IoU (\%)} & \textbf{F1 (\%)} & \textbf{P (\%)} & \textbf{R (\%)} \\
    \midrule
    Standard & 81.30 $\pm$ 0.22 & 89.69 $\pm$ 0.14 & 87.81 $\pm$ 0.12 & 91.64 $\pm$ 0.16 \\
    Co-Teaching & 82.00 $\pm$ 0.34 & 90.11 $\pm$ 0.21 & \textbf{88.93 $\pm$ 0.21} & 91.32 $\pm$ 0.21 \\
    JoCoR & 82.13 $\pm$ 0.15 & 90.19 $\pm$ 0.09 & 88.08 $\pm$ 0.19 & 92.39 $\pm$ 0.11 \\
    CORES$^2$ & 81.54 $\pm$ 0.20 & 89.83 $\pm$ 0.13 & 88.00 $\pm$ 0.17 & 91.74 $\pm$ 0.08 \\
    Peer Loss & 81.41 $\pm$ 0.33 & 89.75 $\pm$ 0.20 & \underline{88.82 $\pm$ 0.24} & 90.70 $\pm$ 0.15 \\
    Bootstrap & 81.94 $\pm$ 0.31 & 90.07 $\pm$ 0.19 & 88.23 $\pm$ 0.26 & 92.00 $\pm$ 0.13 \\
    COPLE & 81.67 $\pm$ 0.18 & 89.91 $\pm$ 0.11 & 88.26 $\pm$ 0.18 & 91.63 $\pm$ 0.14 \\
    ADELE & 82.11 $\pm$ 0.19 & 90.17 $\pm$ 0.12 & 88.30 $\pm$ 0.19 & 92.13 $\pm$ 0.07 \\
    L2B & 71.77 $\pm$ 0.07 & 83.57 $\pm$ 0.05 & 75.21 $\pm$ 0.06 & \textbf{94.01 $\pm$ 0.03} \\
    AIO2 & \underline{82.35 $\pm$ 0.08} & \underline{90.32 $\pm$ 0.05} & 88.36 $\pm$ 0.14 & 92.38 $\pm$ 0.08 \\
    \midrule
    \textbf{ACOC-MT (Ours)} & \textbf{82.53 $\pm$ 0.18} & \textbf{90.42 $\pm$ 0.11} & 88.40 $\pm$ 0.36 & \underline{92.55 $\pm$ 0.22} \\
    \bottomrule
\end{tabular}
}
\end{table*} 

\begin{table*}[h!]
\centering
\caption{Performance Comparison on the BBD250-N Dataset. All metrics are presented as Mean ± Std (\%). The \textbf{best} and \underline{second-best} results are highlighted.}
\label{tab:method_comparison_bbd250}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{l c c c c}
    \toprule
    \textbf{Method} & \textbf{IoU (\%)} & \textbf{F1 (\%)} & \textbf{P (\%)} & \textbf{R (\%)} \\
    \midrule
    Standard & 86.41 $\pm$ 0.11 & 92.71 $\pm$ 0.07 & 96.14 $\pm$ 0.09 & 89.51 $\pm$ 0.09 \\
    Co-Teaching & 86.61 $\pm$ 0.16 & 92.82 $\pm$ 0.09 & 96.15 $\pm$ 0.03 & 89.72 $\pm$ 0.14 \\
    JoCoR & 86.86 $\pm$ 0.25 & 92.97 $\pm$ 0.14 & 96.28 $\pm$ 0.05 & 89.88 $\pm$ 0.26 \\
    CORES$^2$ & 86.76 $\pm$ 0.19 & 92.91 $\pm$ 0.11 & 95.25 $\pm$ 0.14 & 90.69 $\pm$ 0.10 \\
    Peer Loss & 87.48 $\pm$ 0.21 & 93.32 $\pm$ 0.12 & 96.69 $\pm$ 0.05 & 90.18 $\pm$ 0.20 \\
    Bootstrap & 87.01 $\pm$ 0.21 & 93.06 $\pm$ 0.12 & 95.43 $\pm$ 0.11 & 90.80 $\pm$ 0.16 \\
    COPLE & 86.76 $\pm$ 0.16 & 92.91 $\pm$ 0.09 & 95.78 $\pm$ 0.02 & 90.21 $\pm$ 0.15 \\
    ADELE & 79.42 $\pm$ 0.24 & 88.53 $\pm$ 0.15 & \underline{97.05 $\pm$ 0.04} & 81.38 $\pm$ 0.24 \\
    L2B & 67.12 $\pm$ 0.06 & 80.32 $\pm$ 0.04 & \textbf{97.11 $\pm$ 0.04} & 68.49 $\pm$ 0.07 \\
    AIO2 & \underline{87.75 $\pm$ 0.17} & \underline{93.48 $\pm$ 0.10} & 95.55 $\pm$ 0.05 & \underline{91.49 $\pm$ 0.15} \\
    \midrule
    \textbf{ACOC-MT (Ours)} & \textbf{89.15 $\pm$ 0.08} & \textbf{94.26 $\pm$ 0.05} & 96.60 $\pm$ 0.04 & \textbf{92.03 $\pm$ 0.07} \\
    \bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[h!]
\centering
\caption{Performance Comparison on the CFD-N Dataset. All metrics are presented as Mean ± Std (\%). The \textbf{best} and \underline{second-best} results are highlighted.}
\label{tab:method_comparison_cfd}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{l c c c c}
    \toprule
    \textbf{Method} & \textbf{IoU (\%)} & \textbf{F1 (\%)} & \textbf{P (\%)} & \textbf{R (\%)} \\
    \midrule
    Standard & 70.00 $\pm$ 0.19 & 82.36 $\pm$ 0.13 & 84.40 $\pm$ 0.10 & 80.72 $\pm$ 0.32 \\
    Co-Teaching & 70.06 $\pm$ 0.34 & 82.40 $\pm$ 0.24 & 79.93 $\pm$ 0.28 & \textbf{84.88 $\pm$ 0.25} \\
    JoCoR & 70.33 $\pm$ 0.14 & 82.57 $\pm$ 0.11 & 91.77 $\pm$ 0.07 & 75.07 $\pm$ 0.22 \\
    CORES$^2$ & 70.94 $\pm$ 0.19 & 83.00 $\pm$ 0.13 & 86.15 $\pm$ 0.21 & 80.08 $\pm$ 0.08 \\
    Peer Loss & 69.90 $\pm$ 0.54 & 82.28 $\pm$ 0.43 & 86.57 $\pm$ 0.21 & 78.40 $\pm$ 0.36 \\
    Bootstrap & 70.22 $\pm$ 0.27 & 82.48 $\pm$ 0.19 & 86.76 $\pm$ 0.04 & 78.64 $\pm$ 0.37 \\
    COPLE & 69.37 $\pm$ 0.34 & 81.91 $\pm$ 0.25 & 88.38 $\pm$ 0.18 & 76.33 $\pm$ 0.37 \\
    ADELE & 65.58 $\pm$ 0.43 & 79.21 $\pm$ 0.30 & \textbf{95.42 $\pm$ 0.06} & 67.70 $\pm$ 0.42 \\
    L2B & 48.74 $\pm$ 0.12 & 65.57 $\pm$ 0.15 & \underline{93.12 $\pm$ 0.09} & 50.60 $\pm$ 0.18 \\
    AIO2 & \underline{71.55 $\pm$ 0.30} & \underline{83.42 $\pm$ 0.23} & 87.11 $\pm$ 0.18 & 80.03 $\pm$ 0.21 \\
    \midrule
    \textbf{ACOC-MT (Ours)} & \textbf{73.94 $\pm$ 0.24} & \textbf{85.02 $\pm$ 0.16} & 87.35 $\pm$ 0.44 & \underline{82.81 $\pm$ 0.10} \\
    \bottomrule
\end{tabular}
}
\end{table*}

The results for the CFD-N dataset are shown in Table \ref{tab:method_comparison_cfd}, which indicate that our proposed ACOC-MT achieves the best overall performance, attaining the highest scores in both IoU (73.94\%) and F1-score (85.02\%).  

The noise in this dataset, originating from pixel-level pseudo-labels, are characterized by a high noise level, poor label quality, and a substantial number of false negatives. In this case, many conventional noise-robust techniques, such as Co-Teaching, JoCoR, and Bootstrap, exhibit only average performance, while Peer Loss and COPLE show a decline, highlighting their limitations in handling the complex noise profiles found in remote sensing imagery.
Methods like ADELE and L2B achieve the highest and second-highest Precision scores but suffer from extremely low Recall. This indicates a highly conservative prediction strategy where the models only segment the most high-confidence objects while missing a large number of true positives. Conversely, Co-Teaching achieves the highest Recall but at the cost of lower Precision, suggesting it is effective at finding most objects but may introduce more false positives.
Only AIO2 and our ACOC-MT demonstrate a strong balance between Precision and Recall, leading to the top two performances in the crucial IoU and F1 metrics. By leverages early learning detection to activate label correction before the model overfits noise labels while allowing the model to preserve the ability to identify missed noise objects, our ACOC-MT method achieves the optimal balance and, consequently, the best overall performance.


A particularly noteworthy result across all three of our real-world datasets is the poor performance of L2B. We attribute this to a fundamental mismatch between L2B's guiding mechanism and the nature of real-world remote sensing noise.
L2B's core mechanism is designed to be guided by a small, clean, and unbiased set to learn per-sample weights. When such a clean set is unavailable, as is the case in our experiments, L2B's strategy is to use a Gaussian Mixture Model (GMM) to dynamically separate training samples into "clean" and "noisy" sets based on their loss distribution. This estimated "clean" set then guides the complex optimization process.
However, the underlying assumption that clean and noisy samples form separable distributions in the loss space may not hold in complex remote sensing scenarios. For instance, a large, incorrectly labeled object (a false positive) can exhibit a low loss early in training and be misclassified by the GMM as "clean." Conversely, a small or boundary-complex true object (a hard sample) might consistently produce a high loss and be misclassified as "noisy."
When the GMM provides such a flawed reference set, the dynamic weighting process is guided by incorrect signals, leading to suboptimal weight assignments and skewed performance. The underperformance of L2B thus highlights the unique challenges of real-world remote sensing noise and underscores the value of our more direct, consistency-based correction approach.


Overall, based on experiments conducted on three real-world noisy labeled datasets from different sources, we draw the following conclusions:


\begin{itemize}
\item Due to the diverse and complex nature of real-world noisy labels in remote sensing, the experimental performance of various methods is generally less impressive compared to their results on synthetic noisy datasets. Some methods even experience performance degradation, further reinforcing the need for real-world noisy-labeled datasets and more specialized methods.
\item Datasets with noise from coarse annotations tend to perform better with label correction methods. Datasets derived from crowdsourced data tend to perform better with noise-robust regularization methods. The most challenging case arises from pixel-level pseudo-labels, which require models with strong generalization capabilities.
\item Although our method does not show the highest precision or recall in some cases, it consistently achieves the best overall performance (for IoU and F1) across all three datasets, demonstrating the superior generalization ability of ACOC-MT.
\end{itemize}


\textbf{Qualitative Results}
We provide two types of qualitative results to visually validate the effectiveness of our proposed ACOC-MT framework. 

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.95\textwidth]{fig6.png} 
  \caption{Qualitative comparison of ACOC-MT and other methods across three datasets. ACOC-MT achieves more accurate segmentation by effectively correcting mislabeled regions while maintaining structural integrity, demonstrating superior generalizability across diverse noise sources.}
  \label{fig:buchong_2}
\end{figure*}

First, we present qualitative comparisons against other state-of-the-art approaches in Fig. \ref{fig:buchong_2}. While these results are from representative samples and showcase the superior performance of our method in generating cleaner and more accurate segmentation masks, we acknowledge that a small subset of visualizations may not fully capture the performance across the entire dataset.



Second, to better illustrate our method's correction capabilities, Fig. \ref{fig:5} provides a direct visual comparison between the original noisy labels, our corrected labels, and the ground truth. The visual examples demonstrate the versatility of our approach across different noise types. For instance, in the CPCTC-N dataset, our method effectively removes false positives such as shadows. On the BBD250-N dataset, it successfully refines noisy labels from crowdsourced data while preserving high-quality object boundaries. Furthermore, for the CFD-N dataset, it recovers large, contiguous areas of missed detections originating from the pseudo-labels.

\begin{table*}[b]
\centering
\caption{Performance Comparison (Mean ± Std (\%)) under Artificial Noise on the Massachusetts Buildings Dataset. For each noise level, the \textbf{best} and \underline{second-best} results among all noise-robust methods are highlighted.}
\label{tab:engineered_noise_full}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|ccc|ccc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{IoU (\%)}} & \multicolumn{3}{c}{\textbf{F1 (\%)}} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7}
    & \textbf{30\% Noise} & \textbf{50\% Noise} & \textbf{70\% Noise} & \textbf{30\% Noise} & \textbf{50\% Noise} & \textbf{70\% Noise} \\
    \midrule
    Standard & 60.04 $\pm$ 0.70 & 44.55 $\pm$ 0.26 & 20.06 $\pm$ 0.12 & 75.03 $\pm$ 0.55 & 61.64 $\pm$ 0.25 & 33.41 $\pm$ 0.16 \\
    \midrule
    Co-Teaching & 63.80 $\pm$ 1.23 & 55.02 $\pm$ 0.81 & 22.68 $\pm$ 0.59 & 77.90 $\pm$ 0.92 & 70.98 $\pm$ 0.69 & 36.98 $\pm$ 0.79 \\
    JoCoR & 61.04 $\pm$ 0.84 & 56.82 $\pm$ 0.48 & 31.39 $\pm$ 0.30 & 75.80 $\pm$ 0.64 & 72.47 $\pm$ 0.38 & 47.78 $\pm$ 0.34 \\
    CORES$^2$ & 61.54 $\pm$ 0.47 & 50.14 $\pm$ 0.63 & 21.50 $\pm$ 2.05 & 76.20 $\pm$ 0.36 & 66.79 $\pm$ 0.55 & 35.36 $\pm$ 2.69 \\
    Peer Loss & 37.87 $\pm$ 0.95 & 38.45 $\pm$ 0.36 & 16.87 $\pm$ 0.10 & 54.93 $\pm$ 0.98 & 55.51 $\pm$ 0.43 & 28.87 $\pm$ 0.13 \\
    Bootstrap & 60.06 $\pm$ 0.25 & 45.08 $\pm$ 0.27 & 25.59 $\pm$ 0.67 & 75.05 $\pm$ 0.20 & 62.14 $\pm$ 0.26 & 40.75 $\pm$ 0.82 \\
    COPLE & 63.77 $\pm$ 0.49 & \underline{63.49 $\pm$ 0.22} & 30.72 $\pm$ 1.21 & 77.88 $\pm$ 0.37 & \underline{77.67 $\pm$ 0.15} & 46.99 $\pm$ 1.39 \\
    ADELE & 62.63 $\pm$ 0.43 & 62.23 $\pm$ 0.74 & 28.37 $\pm$ 2.76 & 76.99 $\pm$ 0.32 & 76.71 $\pm$ 0.56 & 44.16 $\pm$ 3.39 \\
    L2B & 55.52 $\pm$ 0.02 & 52.63 $\pm$ 2.07 & 29.31 $\pm$ 5.23 & 69.94 $\pm$ 2.21 & 68.95 $\pm$ 1.76 & 45.17 $\pm$ 6.13 \\
    AIO2 & \underline{66.88 $\pm$ 0.92} & 62.23 $\pm$ 0.14 & \underline{33.03 $\pm$ 2.07} & \underline{80.15 $\pm$ 0.67} & 76.71 $\pm$ 0.11 & \underline{49.64 $\pm$ 2.33} \\
    \midrule
    \textbf{ACOC-MT (Ours)} & \textbf{67.21 $\pm$ 0.22} & \textbf{65.85 $\pm$ 0.47} & \textbf{38.71 $\pm$ 2.53} & \textbf{80.39 $\pm$ 0.15} & \textbf{79.41 $\pm$ 0.33} & \textbf{55.66 $\pm$ 2.82} \\
    \bottomrule
\end{tabular}
}
\end{table*}

These visual results highlight our framework's ability to effectively handle the specific challenges posed by each noise type and reinforces the versatility of our label correction method.

\begin{figure}[t]
\normalsize
  \centering
   \includegraphics[width=1\linewidth]{fig7.pdf}
   \caption{The qualitative results of ACOC-MT label correction are shown for three different datasets: (a) the CPCTC-N dataset, (b) the BBD250-N dataset, and (c) the CFD-N dataset. These results demonstrate the applicability of our method across various remote sensing label noise scenarios.}
   \label{fig:5}
\end{figure}



\subsection{Generalization and Robustness Analysis}

To further assess the capabilities of our ACOC-MT framework beyond our curated datasets, we conducted two additional sets of experiments. First, we evaluated its robustness in a well-controlled study with artificial noise. Second, we tested its generalizability on a challenging public benchmark for multi-class segmentation.

\textbf{Robustness to Artificial Noise} To create a well-controlled experimental setting, we used the clean labels of the Massachusetts Buildings Dataset and simulated "incomplete label noise" by randomly dropping 30\%, 50\%, and 70\% of the building instances from the training set. We then trained our ACOC-MT and a representative set of baseline methods on these synthetically corrupted labels and evaluated them on the clean validation set. The results are presented in Table \ref{tab:engineered_noise_full}.



First, it is evident that the performance of all methods degrades as the noise ratio increases, with the standard baseline suffering a catastrophic drop at the 70\% noise level, demonstrating the necessity of a robust learning strategy. In contrast, our ACOC-MT framework significantly outperforms all other methods across all three noise levels.

Second, a deeper analysis reveals that the performance advantage of our ACOC-MT becomes more pronounced as the noise ratio intensifies. At 30\% noise, ACOC-MT outperforms the second-best method (AIO2) by 0.33\% in IoU. This gap widens to 2.36\% over the second-best method (COPLE) at 50\% noise, and further increases to a significant 5.68\% advantage over AIO2 at the extreme 70\% noise level.

Finally, we observe that at 70\% noise, many conventional methods essentially collapse, with their performance approaching that of the standard baseline. In contrast, methods that perform active label correction, such as AIO2 and our own, show greater resilience. The superior performance of ACOC-MT in this high-noise regime highlights the effectiveness of our CTM module's self-consistency check, which can provide a more reliable signal for correction when a majority of the labels are missing.



\textbf{Generalization to Multi-Class Segmentation} To demonstrate the generalizability of our ACOC-MT framework, we evaluated it on the challenging, large-scale NoLDO-S12 public benchmark, a multi-class semantic segmentation task. As detailed in Section IV-A, we used a randomly sampled subset of the noisy `SSL4EO-S12@NoL` (using only Sentinel-2 RGB data) as the training set and the clean `SSL4EO-S12@DW` as the test set. For comparison, we selected a set of representative methods: a standard training baseline; ADELE, a strong method originally validated on multi-class segmentation tasks; and the most relevant state-of-the-art method, AIO2, which we adapted for the multi-class scenario by modifying its final classification layer.

The overall results are summarized in Table \ref{tab:overall_performance}. Despite the challenging nature of this dataset, our ACOC-MT framework achieves the highest overall mIoU and F1-score, demonstrating its superior and robust performance on this public multi-class benchmark.

\begin{table}[h!]
\centering
\caption{PERFORMANCE COMPARISON ON THE NoLDO-S12 DATASET. ALL METRICS ARE PRESENTED AS MEAN ± STD (\%). THE BEST RESULTS ARE HIGHLIGHTED.}
\label{tab:overall_performance}
\resizebox{0.45\textwidth}{!}{%
    \begin{tabular}{lcc}
        \toprule
        \textbf{Method} & \textbf{mIoU (\%)} & \textbf{Overall F1 (\%)} \\
        \midrule
        Standard & 34.82$\pm$0.19 & 48.20$\pm$0.29 \\
        ADELE & 35.60$\pm$0.07 & 49.19$\pm$0.23 \\
        AIO2 & 31.49$\pm$0.35 & 44.48$\pm$0.56 \\
        \midrule
        \textbf{ACOC-MT (Ours)} & \textbf{36.31$\pm$0.29} & \textbf{49.51$\pm$0.31} \\
        \bottomrule
    \end{tabular}
}
\end{table}

\begin{table*}[b]
\centering
\caption{Detailed Performance Comparison on the NoLDO-S12 Dataset (Mean$\pm$Std (\%)). For the remaining classes, the best IoU and F1 scores are independently highlighted in \textbf{bold}.}
\label{tab:merged_final_compact}

% --- 优化措施：减小列间距以优化布局 ---
\setlength{\tabcolsep}{4pt}

\resizebox{\textwidth}{!}{%
\begin{tabular}{ll|cccccccc}
    \toprule
    \textbf{Metric} & \textbf{Method} & \textbf{Water} & \textbf{Trees} & \textbf{Grass} & \textbf{Crops} & \textbf{Shrub \& scrub} & \textbf{Built} & \textbf{Bare} & \textbf{Snow \& ice} \\
    \midrule
    \multirow{4}{*}{\textbf{IoU (\%)}} & Standard & 39.66$\pm$2.50 & 55.40$\pm$3.39 & \textbf{16.90$\pm$0.75} & 31.65$\pm$0.23 & 24.20$\pm$2.42 & 51.74$\pm$2.26 & 36.02$\pm$1.83 & 62.53$\pm$0.77 \\
    & ADELE & 36.48$\pm$1.13 & 51.37$\pm$2.37 & 14.14$\pm$2.01 & 34.48$\pm$0.68 & \textbf{27.45$\pm$0.21} & \textbf{56.28$\pm$0.13} & 39.14$\pm$1.54 & 60.40$\pm$0.12 \\
    & AIO2 & 33.36$\pm$5.37 & 47.82$\pm$8.57 & 8.08$\pm$2.39 & 34.04$\pm$1.11 & 24.24$\pm$1.05 & 49.26$\pm$1.76 & 32.17$\pm$4.32 & 56.10$\pm$2.84 \\
    & \textbf{ACOC-MT (Ours)} & \textbf{42.02$\pm$0.98} & \textbf{58.12$\pm$1.27} & 13.72$\pm$0.88 & \textbf{35.16$\pm$0.73} & 26.19$\pm$0.51 & 54.96$\pm$0.51 & \textbf{39.31$\pm$1.01} & \textbf{62.80$\pm$0.13} \\
    \midrule
    \multirow{4}{*}{\textbf{F1 (\%)}} & Standard & 56.77$\pm$2.50 & 71.25$\pm$2.87 & \textbf{28.91$\pm$1.09} & 48.08$\pm$0.25 & 38.92$\pm$3.03 & 68.18$\pm$1.94 & 52.94$\pm$1.98 & 76.94$\pm$0.58 \\
    & ADELE & 53.44$\pm$1.25 & 67.85$\pm$2.06 & 24.73$\pm$3.09 & 51.27$\pm$0.73 & \textbf{43.07$\pm$0.27} & \textbf{72.02$\pm$0.10} & 56.25$\pm$1.59 & 75.31$\pm$0.10 \\
    & AIO2 & 49.87$\pm$6.00 & 64.39$\pm$8.12 & 14.88$\pm$4.14 & 50.79$\pm$1.21 & 39.00$\pm$1.34 & 66.00$\pm$1.60 & 48.67$\pm$4.96 & 71.71$\pm$2.33 \\
    & \textbf{ACOC-MT (Ours)} & \textbf{59.17$\pm$0.96} & \textbf{73.51$\pm$1.01} & 24.13$\pm$1.37 & \textbf{52.02$\pm$0.82} & 41.51$\pm$0.64 & 70.93$\pm$0.43 & \textbf{56.43$\pm$1.05} & \textbf{77.15$\pm$0.10} \\
    \bottomrule
\end{tabular}
}
\end{table*}

A detailed per-class analysis in Table \ref{tab:merged_final_compact} provides deeper insights and reveals the underlying strengths of our approach. We exclude the 'Flooded veg.' class from this per-class analysis because its extreme rarity in the dataset (<2.5\% of samples) leads to statistically unreliable scores that are not informative for comparing model performance. Our method's advantage is broad-based, achieving the best performance in most key categories such as 'Water', 'Trees', and 'Crops'. A particularly noteworthy result is the performance of AIO2. While a strong competitor on its native binary, incomplete-label task, its performance degrades significantly in this more complex, multi-class scenario, falling below even the standard baseline. This suggests that its specialized correction mechanism, which is designed to add omitted objects, does not generalize well to the diverse noise types (e.g., inter-class confusion) present in this dataset. In contrast, our ACOC-MT leverages a more fundamental and class-agnostic consistency-based correction principle, leading to the best performance.





\subsection{Ablation Study}

\textbf{Effectiveness of the Proposed Modules}
We conducted an ablation study by applying ACOC-MT to the CFD-N dataset, using U-Net as the network architecture for our experiments. The results are shown in Table \ref{tab:method_comparison_iou}. The NONE configuration serves as a baseline, where early learning detection is fixed at 30 epochs, the label selection strategy randomly selects 50\% of the samples for updates, and the label correction strategy performs pixel-level full replacement. We observe that inadequate handling of aspects such as when to select labels, which labels to select, and how to handle the selected labels can even lead to a decrease in model accuracy. However, with the gradual incorporation of our modules, model performance steadily improves. 
This confirms: (1) that carefully addressing these key factors can significantly optimize model training and results, and (2) that the effectiveness of these modules in enhancing the model’s ability to handle noisy labels is demonstrated.

\begin{table}[h!]
\centering
\normalsize
\caption{The ablation study of our approach. The BASE method refers to directly training U-Net on the noisy dataset without any additional modules. The NONE configuration employs a fixed label correction strategy. In subsequent experiments, we progressively incorporate the SELD, CTM, and OLC modules, which lead to a gradual improvement in performance.}
\begin{tabular}{lcccc}
    \toprule
    \textbf{Methods} & \textbf{SELD} & \textbf{CTM} & \textbf{OLC} & \textbf{IoU (\%)} \\
    \midrule
    BASE & & & & 70.00 \\
    \midrule
    NONE & & & & 68.92 \textcolor{blue}{$\downarrow$ 1.08} \\
    + SELD & \checkmark & & & 72.01 \textcolor{red}{$\uparrow$ 3.09} \\
    + CTM & \checkmark & \checkmark & & 72.35 \textcolor{red}{$\uparrow$ 0.34} \\
    + OLC & \checkmark & \checkmark & \checkmark & 73.94 \textcolor{red}{$\uparrow$ 1.59} \\
    \bottomrule
\end{tabular}
\label{tab:method_comparison_iou}
\end{table}

\begin{table}[h!]
\centering
\caption{Ablation Study on the CTM Split Configuration on the CFD-N Dataset. IoU is presented as Mean ± Std (\%). Throughput is measured in Frames Per Second (FPS).}
\label{tab:ablation_split}
\resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c c c}
        \toprule
        \textbf{Split Configuration} & \textbf{IoU (\%)} & \textbf{Throughput (FPS)} \\
        \midrule
        1x1 & 70.00 $\pm$ 0.19 & 38.77 \\
        2x2 & \textbf{73.94 $\pm$ 0.24} & \textbf{9.59} \\
        3x3 & 72.16 $\pm$ 0.86 & 4.27 \\
        \bottomrule
    \end{tabular}
}

\end{table}

\textbf{Ablation Study on Split Configuration}
To validate our choice of a 2x2 split for the CTM module, we conducted an ablation study comparing 1x1, 2x2, and 3x3 configurations. It is important to note that the 1x1 configuration serves as our baseline, as the framework logically degenerates to standard training on noisy labels without a split to activate the CTM consistency check. We did not explore finer splits (e.g., 4x4) as the 3x3 configuration already showed diminishing returns in accuracy at a significantly higher computational cost.
The results of this comparison are presented in Table \ref{tab:ablation_split}. It is clear that the 2x2 split achieves the highest IoU. The 3x3 split, while also outperforming the baseline, yields a lower IoU than the 2x2 split and suffers from a significant decrease in processing speed, as measured by throughput (FPS). This confirms that the 2x2 split provides the optimal balance between performance and computational efficiency for our method.





\begin{table*}[b!]
\centering
\caption{Ablation Study on the Label Handling Strategy on the CFD-N Dataset. All metrics are presented as Mean ± Std (\%).}
\label{tab:ablation_label_strategy}
\resizebox{0.95\textwidth}{!}{
    \begin{tabular}{l r @{\,} l r @{\,} l r @{\,} l r @{\,} l}
        \toprule
        % Use \multicolumn{2}{c}{...} to make each header span two columns (the result column and the improvement column)
        \textbf{Strategy} & \multicolumn{2}{c}{\textbf{IoU (\%)}} & \multicolumn{2}{c}{\textbf{F1 (\%)}} & \multicolumn{2}{c}{\textbf{P (\%)}} & \multicolumn{2}{c}{\textbf{R (\%)}} \\
        \midrule
        Label Dropping & 70.39 $\pm$ 1.15 && 82.62 $\pm$ 0.79 && 86.96 $\pm$ 4.72 && 78.89 $\pm$ 2.64 & \\
        Label Correction (Ours) & 73.94 $\pm$ 0.24 & \textcolor{red}{\,$\uparrow$ 3.55} & 85.02 $\pm$ 0.16 & \textcolor{red}{\,$\uparrow$ 2.40} & 87.35 $\pm$ 0.44 & \textcolor{red}{\,$\uparrow$ 0.39} & 82.81 $\pm$ 0.10 & \textcolor{red}{\,$\uparrow$ 3.92} \\
        \bottomrule
    \end{tabular}
}
\end{table*}

\textbf{Analysis of Label Handling Strategy} To further validate our label correction strategy, we conducted an ablation study comparing our OLC module's approach (Label Correction) with a common alternative: dropping low-consistency labels entirely (Label Dropping). This experiment aims to demonstrate the benefit of our data-efficient correction approach over a more aggressive sample selection strategy.

As shown in Table \ref{tab:ablation_label_strategy}, our method achieves a net gain of 3.55\% in mean IoU, indicating a substantial improvement in overall segmentation quality. More revealing is the significant 3.92\% increase in Recall. This highlights that our method is more effective at identifying all ground truth objects, significantly reducing the rate of False Negatives. This finding substantiates our core design rationale: by salvaging and refining supervisory signals from difficult or noisy examples, rather than simply discarding them, our correction strategy enables the model to learn more robust and generalizable features, ultimately achieving more accurate and stable results.





\textbf{Framework Generalizability Across Architectures}
Our framework is plug-and-play and compatible with any standard semantic segmentation network. To demonstrate that our method is network-agnostic and to further validate its generalization, we conducted experiments on the CFD-N dataset using not only U-Net but also UperNet \cite{xiao2018unified} and Mask2Former \cite{cheng2022masked}. UperNet represents a typical CNN-based cascade architecture, while Mask2Former serves as a representative transformer-based segmentation model.

The experimental results, as shown in Table \ref{tab:results}, demonstrate the effectiveness of our ACOC-MT framework across these different architectures. Specifically, applying our method improved the IoU on the U-Net network by 3.94\%. For UperNet, the IoU increased by 2.07\%, while Mask2Former achieved a notable 7.29\% improvement. These results indicate that our ACOC-MT framework effectively enhances semantic segmentation performance under noisy labels across various network architectures.

An interesting observation is the significantly larger performance improvement when applying ACOC-MT to the Mask2Former backbone (a 7.29\% gain in IoU) compared to the CNN-based backbones (2-4\% gains). We speculate that this is due to the inherent architectural differences. Transformer-based models like Mask2Former utilize a global self-attention mechanism, making them exceptionally adept at capturing long-range dependencies and leveraging global image context. Our CTM module's core principle is to quantify reliability based on the stability of predictions under changes in this very context (global vs. local). Therefore, a model that is inherently more powerful at processing global context, like Mask2Former, is likely to benefit more substantially from our context-aware correction signal.

\begin{table}[h!]
\centering
\normalsize
\caption{Performance comparison of different models with and without ACOC-MT.}
\label{tab:results}
\resizebox{0.48\textwidth}{!}{%
    \begin{tabular}{llcc}
        \toprule
        \textbf{Base Model} & \textbf{Method} & \multicolumn{2}{c}{\textbf{IoU (\%)}} \\
        \midrule
        \multirow{2}{*}{U-Net} & Baseline & 70.00 $\pm$ 0.19 & \\
                                 & + ACOC-MT & 73.94 $\pm$ 0.24 & \textcolor{red}{$\uparrow$ 3.94}  \\
        \midrule
        \multirow{2}{*}{UperNet} & Baseline & 67.02 $\pm$ 0.21 & \\
                                   & + ACOC-MT & 69.09 $\pm$ 0.18 & \textcolor{red}{$\uparrow$ 2.07} \\
        \midrule
        \multirow{2}{*}{Mask2Former} & Baseline & 60.85 $\pm$ 0.25 & \\
                                         & + ACOC-MT & 68.14 $\pm$ 0.22 & \textcolor{red}{$\uparrow$ 7.29} \\
        \bottomrule
    \end{tabular}
}
\end{table}



\subsection{Parameter Sensitivity Analysis}
Our proposed method, ACOC-MT, requires the setting of two hyperparameters: the candidate point threshold $r$ and the accumulated time of candidate points $m$. We investigated the impact of these two parameters on the experimental results by applying ACOC-MT to the CFD-N dataset. 
In our sensitivity analysis for the deceleration threshold $r$, we focus on the range $[0.75, 1.0]$. This range corresponds to the critical phase where the model is approaching convergence. In contrast, values of $r$ below 0.75 would trigger the stopping mechanism prematurely, when the model is still significantly underfitted. Using the highly unreliable model state from this early stage leads to a collapse in performance, rendering the analysis of this lower range uninformative.
The results are shown in Fig.\ref{fig:8}.

\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{fig8.pdf}
   \caption{The Parameter Sensitivity Analysis. For the left panel (a), we fixed  $r$  at 0.9 and varied  $m$  from 1 to 14 to observe the performance changes. For the right panel (b), we fixed  $m$  at 10 and adjusted  $r$  from 0.75 to 0.99 to analyze the impact on performance. This setup allows us to evaluate how the two hyperparameters  $r$  and  $m$  affect the model’s performance, providing insight into the optimal settings for each parameter. The reported metric is the Intersection over Union (IoU), calculated exclusively on the foreground class.}
   \label{fig:8}
\end{figure}

Our experiments demonstrate that the proposed method exhibits low sensitivity to the hyperparameters $r$ and $m$. Across a broad range of values for these parameters, the method’s performance remains stable, suggesting that it is robust and does not heavily rely on precise tuning of these settings. This consistency further underscores the generalizability of the approach, indicating its suitability for practical applications without requiring extensive hyperparameter optimization. In practical applications, we recommend setting $m$ to 6 or higher and  $r$  within the range of [0.9, 0.95]. 

\section{Conclusion}
In this paper, we introduced the Adaptive Confidence-Guided Object-Level Label Correction with Mean Teacher (ACOC-MT) framework for semantic segmentation in RS images with noisy labels. By leveraging real-world noisy datasets, including CPCTC-N, BBD250-N, and CFD-N, we demonstrated the effectiveness of our approach in addressing the challenges of label noise. Although our method effectively corrects label noise in diverse real-world scenarios, certain challenges remain. Specifically, its performance on low-noise datasets still requires further improvement.
As part of future work, we aim to explore more adaptive noise modeling strategies to dynamically adjust the correction process. Additionally, incorporating self-supervised or contrastive learning techniques may further enhance the robustness of our approach.
Beyond semantic segmentation, we plan to investigate the applicability of our method to other remote sensing tasks, such as change detection and object detection. We believe this work contributes to advancing label correction techniques for noisy semantic segmentation, providing a reliable solution for processing real-world remote sensing data with varying noise levels.

\section*{Data and Code Availability} % Using subsection* removes the number
The code and datasets presented in this study are available at the following URL: \url{https://github.com/ING96/ACOC-MT}.

\section*{Acknowledgment}
The authors would like to acknowledge the support of the National Key Research and Development Program of China (2021YFB3901202) and express their gratitude to the editor and the anonymous reviewers for their constructive comments, which have significantly improved the quality of this paper.








% \bibliographystyle{IEEEtran123} % 使用 IEEEtran 参考文献样式
% \bibliography{IEEEabrv, main2} % 指定 .bib 文件（无需扩展名）
\printbibliography



\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Zeqing Wang.png}}]{Zeqing Wang}
received his B.S. degree in Computer Science and Technology from China University of Geosciences (Wuhan) in 2022. He is currently pur- suing a Ph.D. degree in Cartography and Geographic Information Systems at the Aerospace Information Research Institute, Chinese Academy of Sciences.

His research interests include deep learning, learn- ing with noisy labels, and intelligent interpretation of remote sensing data.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Yulong Ding.jpg}}]{Yulong Ding}
 received the B.S. degree in Computer Science and Technology from Henan University of Technology, Zhengzhou, China, in 2022, and the M.S. degree in Surveying Engineering from China University of Mining and Technology (Beijing), Beijing, China, in 2025. He is currently pursuing the Ph.D. degree in Geodesy and Surveying Engineering at China University of Mining and Technology (Beijing), China. 

His research interests include intelligent interpretation of remote sensing, machine learning, and applications of geographic information system (GIS) technology.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Yixiang Li.jpg}}]{Yixiang Li}
 received the B.S. degree in Science and Technology of Remote Sensing from Wuhan University in 2022. He is currently working toward the Ph.D. degree in cartography and geographic information systems with the Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China. 
 
 His research interests include deep learning, forest ecological remote sensing, and crop classification.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Zhaoming Wu.jpg}}]{Zhaoming Wu}
received the B.S. degree from China University of Geosciences (Wuhan) in 2021. He is currently pursuing the Ph.D. degree with the University of the Chinese Academy of Sciences. 

His research interests include remote sensing image super-resolution, change detection, and semantic segmentation.
\end{IEEEbiography}
 
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Xuan Yang.jpg}}]{Xuan Yang}
received the B.Sc. degree in remote sensing science and technology from the Capital Normal University, China, in 2016, and the Ph.D. degree in cartography and geography information system from the Aerospace Information Research Institute, Chinese Academy of Sciences, China, in 2022. He is currently a Postdoctoral Researcher with the China Remote Sensing Satellite Ground Station, Aerospace Information Research Institute, Chinese Academy of Sciences.

His research interests include remote sensing deep learning, semantic segmentation, computer vision, and landuse/landcover.

\end{IEEEbiography}
 
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Zhengchao Chen.jpg}}]{Zhengchao Chen}
received the M.S. degree inremote sensing science from Wuhan University, Wuhan, China, in 2002, and the Ph.D. degree inremote sensing science from the Institute of Remote Sensing and Digital Earth, Chinese Academy of Science, Beijing, China, in 2005. He is currently a Research Professor of the Key Laboratory of Remote Sensing and Digital Earth, Aerospace Information Research Institute, Chinese Academy of Sciences. He set up the Cangling AI Research and Development Team, established a million-level remote sensing knowledge sample dataset, developed a whole-system and whole-process remote sensing intelligent information extraction system, and realized the production of more than 30 remote sensing information products throughout China.

His research interests include quantitative processing and intelligent extrac- tion from remote sensing images.

\end{IEEEbiography}

\vfill

\end{document}


